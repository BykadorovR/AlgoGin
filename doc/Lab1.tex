\documentclass{article}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[utf8]{inputenc} % Включаем поддержку UTF8
\usepackage[russian]{babel}  % Включаем пакет для поддержки русского языка  
\usepackage[table]{xcolor}% http://ctan.org/pkg/xcolor
\usepackage[margin=0.5in]{geometry}
\begin{document}
\section{Теоретическая часть}
\newcommand{\partFunc}{1+e^{-2s_j}}
\newcommand{\dd}[2]{\dfrac{\partial#1}{\partial#2}}
Для того чтобы обучить 3-х слойную нейронную сеть с 1 скрытым слоем, воспользуемся методом обратного распространения ошибки. Основная идея работы этого метода состоит в распространении сигналов ошибки от выходов сети к её входам. Пусть $x_1,\cdots,x_n$ - множество входов. Обозначим через $\omega_{jk}$ вес ребра, соединяющего i и j нейроны, $o_j - $ выход j-го нейрона. Так как обучение происходит с учителем, для каждого входного вектора $x_j$ мы знаем правильный ответы сети $t_k, k \in \text{Нейроны выходного слоя}$. Для того, чтобы узнать насколько ответ сети отличается от ожидаемого, будем использовать функцию ошибки - перекрестная энтропия:
\begin{equation}
\label{eq: 1}
E=-\sum_{j=1}^{n}t_jlog(o_j)
\end{equation}
Метод обратного распространения ошибки можно разделить на 2 части:
\vspace{3mm} \\
\textbf{Прямой ход}: \\
Перед обучением сети проинициализируем веса $\omega_{ij}$ и смещения $\omega_{0j}$ случайными малыми значениями
\begin{enumerate}
	\item Подадим на входной слой значения $x_1,\cdots,x_n$
	\item Каждый входной нейрон отправляет полученный сигнал всем нейронам в следующем скрытом слое
	\item Каждый скрытый нейрон суммирует полученные значения, умножает на соответствующие веса
	$s_{j} = \omega_{0j} + \sum_ix_i\omega_{ij}$ и применяет функцию активации к каждому нейрону:
	$$ %1
	f(s_j)=\dfrac{1}{\partFunc} 
	$$
	после чего отправляет получившиеся значения на выходной слой
	\item Каждый выходной нейрон находит взвешенную сумму, как и нейроны скрытого слоя, после чего применяет функцию активации:
	$$
	f(s_j) = \dfrac{e^{s_j}}{\sum_{j=1}^{n}e^{s_j}}
	$$
\end{enumerate}
\textbf{Обратный ход}:
\begin{enumerate}
	\item Для выходного слоя на основе полученных и ожидаемых значений рассчитывается ошибка по формуле (\ref{eq: 1}) и нормируется. Одним из критериев остановки обучения сети можно считать полученное значения.
	\item Для каждого нейрона выходного слоя рассчитывается ошибка:
    \begin{equation} %8
    \label{eq: 2}
	\dd{E}{s_i} = \sum_{j=1}^{n}\dd{E}{o_j}\dd{o_j}{s_i}; \
	\dd{E}{o_j} = -\dd{t_jlog(o_j)}{o_j} = -\frac{t_j}{o_j}	
	\end{equation}
	\begin{equation} %8
	\label{eq: 3}
	\dd{o_j}{s_i}=
	\begin{dcases}
	\dd{\dfrac{e^{s_j}}{\sum_{k}e^{s_k}}}{s_j} = \dfrac{e^{s_j}\sum_ke^{s_k}-(e^{s_j})^2}{(\sum_ke^{s_k})^2} = 
	\dfrac{e^{s_j}}{\sum_ke^{s_k}} - \dfrac{(e^{s_j})^2}{(\sum_ke^{s_k})^2} = o_j(1-o_j), & \text{если}\ i=j \\
	\dd{\dfrac{e^{s_i}}{\sum_{k}e^{s_k}}}{s_j} = \dfrac{-e^{s_i}e^{s_j}}{(\sum_ke^{s_k})^2} = -o_io_j, & \text{если}\ i \neq j
	\end{dcases}
	\end{equation}
	Из (\ref{eq: 2}) и (\ref{eq: 3}) следует:
	\begin{equation} %9
	\label{eq: 4}
	\delta_i = \dd{E}{s_i} = \sum_j{o_it_j-t_i(1-o_i)} = -t_i+t_io_i+o_i\sum_jt_j=-t_i+o_i(t_i+\sum_jt_j) = o_i - t_i
	\end{equation}
	\clearpage
	\item Каждый нейрон скрытого слоя суммирует входящие ошибки (от нейронов в последующем слое) и умножает величину полученной ошибки на производную активационной функции:
	\begin{align*} %10
	\begin{split}
	&\delta_j = \dd{E}{s_j} = \sum_{k \in \ \text{Выходной слой}}\dd{E}{s_k}\dd{s_k}{s_j} \\
	&\dd{s_k}{s_j}=\dd{s_k}{o_j}\dd{o_j}{s_j} ;\ \dd{s_k}{o_j} = \omega_{jk} ;\\
	&\dd{o_j}{s_j} = 
	\dfrac{\partial(\dfrac{1}{\partFunc})}{\partial s_j} = 
	-\dfrac{1}{(\partFunc)^2}\dfrac{\partial (\partFunc)}{\partial s_j} = \\
	&2e^{-2s_j}\dfrac{1}{(\partFunc)^2}
	= \dfrac{\partFunc-1}{(\partFunc)^2} = 2o_j(1-o_j) \\ 
	&\text{Где} \ \dd{E}{s_k} = \delta_k - \text{поправка, вычисленная для k-го узла} = \ (\ref{eq: 4})
	\end{split}	
	\end{align*}
	Таким образом получаем для нейронов выходного слоя ошибку \\ $\delta_j = o_j - t_j$, а для нейронов скрытого слоя $\delta_j = 2o_j(1-o_j)(\sum_k\delta_k\omega_{jk})$ 
	\item Каждый выходной и скрытый нейрон изменяет веса своих связей с нейронами предыдущих слоев по формуле:  $$\omega{ij} = \omega{ij} -\eta\delta_jo_i$$
\end{enumerate}
Данная процедура повторяется для каждого тренировочного набора. Обучение останавливается после обучения всего тестового набора определенное количество раз или по достижении заданной ошибки между ожидаемым и реальным выходами.
\section{Программная реализация}
Программа состоит из нескольких модулей:
\vspace{3mm} \\
\textbf{Библиотека}:
\begin{enumerate}
	\item \textit{Neuron}: хранит информацию о текущем состоянии нейрона, а также ошибку, необходимую для обучения сети. Каждый нейрон имеет свою функцию активации и два списка нейронов: входящие и выходящие, где для каждой пары определен вес ребра между ними.
	\item \textit{Layer}: хранит список нейронов, принадлижащих данному слою, а также его тип: входной, скрытый, выходной. Нейроны смещения закреплены за слоем и представлены как вектор весов для каждого нейрона следующего слоя. Реализует сбор значений с предыдущего слоя и их преобразование с помощью функции активации.
	\item \textit{LayerBinder}: отвечает непосредственно за логику алгоритма, реализует прямой и обратный ход.
	\item \textit{Вспомогательные классы}: отвечают за расчет значений/производных функций; генерацию случайных весов.
\end{enumerate}
\textbf{Приложение}:
В методе Main происходит загрузка тестовых и тренировочных MNIST данных, их парсинг; обработка входных параметров: количество нейронов в скрытом слое, необходимая максимальная ошибка, количество эпох, скорость обучения; вызов библиотечных классов для обучения сети и проверки корректности работы.
\vspace{3mm} \\
\textbf{Тесты}:
Написано 5 тестов: 3 теста отвечают за работу функций активации, функцию ошибки и их производных, 2 оставшихся теста проверяют корректность работы прямой и обратной части алгоритма. С помощью тестов было найдено несколько ошибок в процессе рефакторинга кода.
\section{Результаты}
Нейронная сеть была натренирована с помощью MNIST данных (60.000 тренировочных наборов) и проверена с помощью тестовых данных (10.000 наборов). Использовались различные конфигурации сети:
\begin{center}
	\begin{tabular}{| p{1.7cm} | p{1.5cm} | p{1.3cm} | p{1.5cm} | p{2.5cm} | p{2.5cm} |}
		\hline
		Нейронов в скрытом слое & Макс. ошибка & Кол-во эпох & Скорость обучения & \cellcolor{blue!25} Трен. наборов неправильно & \cellcolor{blue!25} Тест. наборов неправильно \\ \hline
		200 & 0.001 & 25 & 0.01 & 14 & 247 \\ \hline
		200 & 0.001 & 25 & 0.005 & 141 & 273 \\ \hline
		200 & 0.001 & 25 & 0.008 & 34 & 256 \\ \hline
		200 & 0.0005 & 35 & 0.01 & 0 & 239 \\ \hline
		200 & 0.0005 & 35 & 0.005 & 30 & 257 \\ \hline
		200 & 0.0005 & 35 & 0.008 & 3 & 246 \\ \hline
		300 & 0.001 & 25 & 0.01 & 6 & 228 \\ \hline
		300 & 0.001 & 25 & 0.005 & 110 & 262 \\ \hline
		300 & 0.001 & 25 & 0.008 & 19 & 234 \\ \hline
		\rowcolor{blue!25}
		300 & 0.0005 & 35 & 0.01 & 0 & 207 \\
		\hline
	\end{tabular}
\end{center}
Как можно видеть, наиболее хорошо себя показала сеть с последней конфигурацией. \\ 
\vspace{3mm} \\
\textbf{Процент правильных ответов на тренировочном наборе:} 100\% \\
\textbf{Процент правильных ответов на тестовом наборе:} 97.93\%
\end{document}

